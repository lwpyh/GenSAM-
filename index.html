<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Generalizable SAM++</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon_qmul.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Is Instance-specific Manual Prompt Necessary for Semantic Segmentation by Promptable Segmentation?</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://lwpyh.github.io/" target="_blank">Jian Hu</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://jylin8100.github.io/" target="_blank">Jiayi Lin</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://lvgd.github.io/" target="_blank">Weitong Cai</a>,</span>
                  <span class="author-block">
                    <a href="http://www.eecs.qmul.ac.uk/~sgg/" target="_blank">Shaogang Gong</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- <span class="author-block">Queen Mary University of London<br>AAAI 2024</span> -->
                    <span class="author-block">Queen Mary University of London</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                     <!-- ArXiv abstract Link -->
                    <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/2312.07374" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                    <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                    </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/jyLin8100/GenSAM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <style>
            .content {
              font-family: "Times New Roman", Times, serif;
            }
          </style> -->
          <p>
            Promptable Segmentation approaches like the Segment Anything Model (SAM) exhibit superior generalization ability thanks to extensive training on large-scale annotated masks. However, it relies upon instance-specific manual prompts to guide for segmenting interested targets. Manual prompt can be categorized as visual or text prompts, with the former only providing localization information instead of semantic information, while the latter is the opposite. Both of them can cause ambiguity in interpreting the targets. Additionally, manual prompts may not be always accessible in real-world application. In this work, we aim to eliminate the need for manual prompt. The key idea is to employ a Progressive Task Control (PTC) to gradually control the visual prompts generation, using the semantic information given by a task generic text prompt. To that end, we introduce a test-time adaptation mechanism called Generalizable SAM++ (GenSAM++) to automatically generate and optimize visual prompts. In particular, PTC first maps a task-generic text prompt onto image-specific consensus foreground and background heatmaps using vision-language models, acquiring reliable visual prompts for segmentation. Then the consensus heatmaps are iteratively reweighted on the input image, controlling the model to focus on the targets in a coarse-to-fine manner. Crucially, GenSAM++ is training-free and test-time optimized.
          </p>
          <p>
            <img src="static/images/moti_v1.png" alt="teaser" class="center-image blend-img-background"/>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser video-->

<!-- End teaser video -->


<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Framework</h2>
          <div class="level-set has-text-justified">
            <!-- <p>
              placeholder
            </p> -->
          </div>
          <img src="static/images/framework_v5.png" alt="framework" class="center-image"/>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section is-small">
  <div class="container is-max-desktop"> -->
<section class="section hero is-light">
    <div class="container is-full is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Visualization</h2>
          <div class="level-set has-text-justified">
            <h4 class="subtitle has-text-justified">
              Camouflaged Object Detection (COD), Polyp Image Segmentation (PIS), Transparent Object Detection (TOD) and Shadow Detection (SD)
            </h4>
          </div>
          <img src="static/images/vis_specific_1.png" alt="Vis" class="center-image"/>
        </div>
        <div class="content">
        <div class="level-set has-text-justified">
          <h4 class="subtitle has-text-justified">
           Open-Vocabulary semantic Segmentation (OVS)
          </h4>
        </div>
        <img src="static/images/vis_specific_1.png" alt="framework" class="center-image"/>
      </div>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="section is-small">
  <div class="container is-full is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="level-set has-text-justified">
          <h4 class="subtitle has-text-justified">
            Camouflaged Object Detection (COD)
          </h4>
        </div>
        <img src="static/images/result_cod.png" alt="result_cod" class="center-image"/>
      </div>
      <div class="content">
      <div class="level-set has-text-justified">
        <h4 class="subtitle has-text-justified">
          Polyp Image Segmentation (PIS), Object Detection (TOD) and Shadow Detection (SD)
        </h4>
      </div>
      <img src="static/images/result_pis_tod_sd.png" alt="result_pis_tod_sd" class="center-image"/>
    </div>
      <div class="content">
        <div class="level-set has-text-justified">
          <h4 class="subtitle has-text-justified">
            Open-Vocabulary Segmentation (OVS) and Explainability Tasks
          </h4>
        </div>
        <img src="static/images/result_ovs.png" alt="result_ovs" class="center-image"/>
      </div>
    </div>
    </div>
  </div>
</div>
</section>


<!-- Image carousel -->

<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>placeholder</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
